{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4rc1"
    },
    "colab": {
      "name": "2RBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4448Mxj-Acn6",
        "colab_type": "code",
        "outputId": "357294b9-6d00-4222-ca5f-5c1e743bab5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model, datasets\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def tf_xavier_init(fan_in, fan_out, *, const=1.0, dtype=np.float32):\n",
        "    k = const * np.sqrt(6.0 / (fan_in + fan_out))\n",
        "    return tf.random_uniform((fan_in, fan_out), minval=-k, maxval=k, dtype=dtype)\n",
        "\n",
        "\n",
        "def sample_bernoulli(probs):\n",
        "    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
        "\n",
        "\n",
        "def sample_gaussian(x, sigma):\n",
        "    return x + tf.random_normal(tf.shape(x), mean=0.0, stddev=sigma, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TQlAWyAcn-",
        "colab_type": "code",
        "outputId": "1a537c13-fbe2-4188-bb85-a20d950d9103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "\n",
        "\n",
        "trainx = np.genfromtxt(\"/trainafter.csv\", delimiter=\",\")\n",
        "trainy = np.genfromtxt(\"/trainlabel.csv\", delimiter=\",\")\n",
        "\n",
        "# Convert labels to binary since we will use a softmax output\n",
        "print(trainy)\n",
        "\n",
        "data = trainx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYAMcSjdfj-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwAqpcD9m9Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGtJJDyUAcoE",
        "colab_type": "code",
        "outputId": "6ed2672e-2a84-4738-a640-07c3f866a690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "\n",
        "testx = np.genfromtxt(\"/test.csv\", delimiter=\",\")\n",
        "testy = np.genfromtxt(\"/testlabel.csv\", delimiter=\",\")\n",
        "#testy = np.argmax(testy,axis=1)\n",
        "\n",
        "print(classification_report(testy, logreg.predict(testx)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.53      0.56      0.55        41\n",
            "         1.0       0.51      0.49      0.50        39\n",
            "\n",
            "    accuracy                           0.53        80\n",
            "   macro avg       0.52      0.52      0.52        80\n",
            "weighted avg       0.52      0.53      0.52        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEN-C9abfiOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GBRBM(RBM):\n",
        "    def __init__(self, n_visible, n_hidden, sample_visible=False, sigma=1, **kwargs):\n",
        "        self.sample_visible = sample_visible\n",
        "        self.sigma = sigma\n",
        "\n",
        "        RBM.__init__(self, n_visible, n_hidden, **kwargs)\n",
        "\n",
        "    def _initialize_vars(self):\n",
        "        hidden_p = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hidden_bias)\n",
        "        visible_recon_p = tf.matmul(sample_bernoulli(hidden_p), tf.transpose(self.w)) + self.visible_bias\n",
        "\n",
        "        if self.sample_visible:\n",
        "            visible_recon_p = sample_gaussian(visible_recon_p, self.sigma)\n",
        "\n",
        "        hidden_recon_p = tf.nn.sigmoid(tf.matmul(visible_recon_p, self.w) + self.hidden_bias)\n",
        "\n",
        "        positive_grad = tf.matmul(tf.transpose(self.x), hidden_p)\n",
        "        negative_grad = tf.matmul(tf.transpose(visible_recon_p), hidden_recon_p)\n",
        "\n",
        "        def f(x_old, x_new):\n",
        "            return self.momentum * x_old +\\\n",
        "                   self.learning_rate * x_new * (1 - self.momentum) / tf.to_float(tf.shape(x_new)[0])\n",
        "\n",
        "        delta_w_new = f(self.delta_w, positive_grad - negative_grad)\n",
        "        delta_visible_bias_new = f(self.delta_visible_bias, tf.reduce_mean(self.x - visible_recon_p, 0))\n",
        "        delta_hidden_bias_new = f(self.delta_hidden_bias, tf.reduce_mean(hidden_p - hidden_recon_p, 0))\n",
        "\n",
        "        update_delta_w = self.delta_w.assign(delta_w_new)\n",
        "        update_delta_visible_bias = self.delta_visible_bias.assign(delta_visible_bias_new)\n",
        "        update_delta_hidden_bias = self.delta_hidden_bias.assign(delta_hidden_bias_new)\n",
        "\n",
        "        update_w = self.w.assign(self.w + delta_w_new)\n",
        "        update_visible_bias = self.visible_bias.assign(self.visible_bias + delta_visible_bias_new)\n",
        "        update_hidden_bias = self.hidden_bias.assign(self.hidden_bias + delta_hidden_bias_new)\n",
        "\n",
        "        self.update_deltas = [update_delta_w, update_delta_visible_bias, update_delta_hidden_bias]\n",
        "        self.update_weights = [update_w, update_visible_bias, update_hidden_bias]\n",
        "\n",
        "        self.compute_hidden = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hidden_bias)\n",
        "        self.compute_visible = tf.matmul(self.compute_hidden, tf.transpose(self.w)) + self.visible_bias\n",
        "        self.compute_visible_from_hidden = tf.matmul(self.y, tf.transpose(self.w)) + self.visible_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lxy853aAcoG",
        "colab_type": "code",
        "outputId": "d8236402-7dc5-4cbc-f13a-7614a06465c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import BernoulliRBM\n",
        "# The RBM will have 200 hidden units and will run for 40 iterations with CD-1\n",
        "rbm = BernoulliRBM(n_components = 200, n_iter = 100,learning_rate = 0.01,  verbose = True)\n",
        "\n",
        "rbm.fit(data)\n",
        "\n",
        "# Save the weights of the visible and hidden connections\n",
        "np.save(\"/rbm_weights.npy\",rbm.components_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -2917.08, time = 0.01s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -17038.33, time = 0.01s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -5681.44, time = 0.01s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -20686.49, time = 0.01s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -60098.30, time = 0.01s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -22802.92, time = 0.01s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -32560.73, time = 0.01s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -37482.96, time = 0.01s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -35788.14, time = 0.01s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -41552.24, time = 0.01s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -95253.94, time = 0.01s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -46293.55, time = 0.01s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -126342.39, time = 0.01s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -88223.23, time = 0.01s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -80136.38, time = 0.01s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -117875.93, time = 0.01s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -148380.74, time = 0.01s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -138783.47, time = 0.01s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -150883.88, time = 0.01s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -188876.64, time = 0.01s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -107286.78, time = 0.01s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -88857.58, time = 0.01s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -106625.09, time = 0.00s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -168825.06, time = 0.00s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -246093.59, time = 0.00s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -308692.19, time = 0.00s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -32459.31, time = 0.01s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -122587.09, time = 0.01s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -87640.70, time = 0.01s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -191257.73, time = 0.00s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -147708.11, time = 0.00s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -244521.22, time = 0.00s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -225070.43, time = 0.01s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -204628.31, time = 0.01s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -114427.79, time = 0.00s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -62627.74, time = 0.01s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -149567.47, time = 0.00s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -145244.28, time = 0.00s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -324589.94, time = 0.00s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -72082.13, time = 0.01s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -56925.13, time = 0.01s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -423083.08, time = 0.01s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -139232.93, time = 0.01s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -244041.87, time = 0.01s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -150460.51, time = 0.01s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -70758.39, time = 0.01s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -387731.08, time = 0.01s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -420262.46, time = 0.01s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -297856.82, time = 0.01s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -115197.91, time = 0.01s\n",
            "[BernoulliRBM] Iteration 51, pseudo-likelihood = -276255.32, time = 0.01s\n",
            "[BernoulliRBM] Iteration 52, pseudo-likelihood = -107364.13, time = 0.01s\n",
            "[BernoulliRBM] Iteration 53, pseudo-likelihood = -238269.61, time = 0.01s\n",
            "[BernoulliRBM] Iteration 54, pseudo-likelihood = -164087.91, time = 0.01s\n",
            "[BernoulliRBM] Iteration 55, pseudo-likelihood = -163106.13, time = 0.01s\n",
            "[BernoulliRBM] Iteration 56, pseudo-likelihood = -482482.57, time = 0.01s\n",
            "[BernoulliRBM] Iteration 57, pseudo-likelihood = -630621.34, time = 0.01s\n",
            "[BernoulliRBM] Iteration 58, pseudo-likelihood = -354842.80, time = 0.01s\n",
            "[BernoulliRBM] Iteration 59, pseudo-likelihood = -431353.97, time = 0.01s\n",
            "[BernoulliRBM] Iteration 60, pseudo-likelihood = -751282.48, time = 0.01s\n",
            "[BernoulliRBM] Iteration 61, pseudo-likelihood = -504428.22, time = 0.01s\n",
            "[BernoulliRBM] Iteration 62, pseudo-likelihood = -698785.79, time = 0.01s\n",
            "[BernoulliRBM] Iteration 63, pseudo-likelihood = -406924.86, time = 0.01s\n",
            "[BernoulliRBM] Iteration 64, pseudo-likelihood = -323904.04, time = 0.01s\n",
            "[BernoulliRBM] Iteration 65, pseudo-likelihood = -358930.03, time = 0.01s\n",
            "[BernoulliRBM] Iteration 66, pseudo-likelihood = -248985.11, time = 0.01s\n",
            "[BernoulliRBM] Iteration 67, pseudo-likelihood = -453911.95, time = 0.01s\n",
            "[BernoulliRBM] Iteration 68, pseudo-likelihood = -481504.77, time = 0.01s\n",
            "[BernoulliRBM] Iteration 69, pseudo-likelihood = -645308.16, time = 0.01s\n",
            "[BernoulliRBM] Iteration 70, pseudo-likelihood = -625763.77, time = 0.01s\n",
            "[BernoulliRBM] Iteration 71, pseudo-likelihood = -772569.02, time = 0.01s\n",
            "[BernoulliRBM] Iteration 72, pseudo-likelihood = -272226.12, time = 0.01s\n",
            "[BernoulliRBM] Iteration 73, pseudo-likelihood = -791499.46, time = 0.01s\n",
            "[BernoulliRBM] Iteration 74, pseudo-likelihood = -265739.89, time = 0.01s\n",
            "[BernoulliRBM] Iteration 75, pseudo-likelihood = -434186.59, time = 0.01s\n",
            "[BernoulliRBM] Iteration 76, pseudo-likelihood = -218278.15, time = 0.00s\n",
            "[BernoulliRBM] Iteration 77, pseudo-likelihood = -515879.71, time = 0.00s\n",
            "[BernoulliRBM] Iteration 78, pseudo-likelihood = -476956.66, time = 0.00s\n",
            "[BernoulliRBM] Iteration 79, pseudo-likelihood = -486899.40, time = 0.01s\n",
            "[BernoulliRBM] Iteration 80, pseudo-likelihood = -223188.56, time = 0.01s\n",
            "[BernoulliRBM] Iteration 81, pseudo-likelihood = -766366.90, time = 0.00s\n",
            "[BernoulliRBM] Iteration 82, pseudo-likelihood = -218498.28, time = 0.00s\n",
            "[BernoulliRBM] Iteration 83, pseudo-likelihood = -132431.31, time = 0.00s\n",
            "[BernoulliRBM] Iteration 84, pseudo-likelihood = -647939.34, time = 0.00s\n",
            "[BernoulliRBM] Iteration 85, pseudo-likelihood = -802979.67, time = 0.00s\n",
            "[BernoulliRBM] Iteration 86, pseudo-likelihood = -782123.05, time = 0.00s\n",
            "[BernoulliRBM] Iteration 87, pseudo-likelihood = -614194.38, time = 0.01s\n",
            "[BernoulliRBM] Iteration 88, pseudo-likelihood = -844778.10, time = 0.00s\n",
            "[BernoulliRBM] Iteration 89, pseudo-likelihood = -219866.62, time = 0.00s\n",
            "[BernoulliRBM] Iteration 90, pseudo-likelihood = -225235.75, time = 0.00s\n",
            "[BernoulliRBM] Iteration 91, pseudo-likelihood = -625665.85, time = 0.00s\n",
            "[BernoulliRBM] Iteration 92, pseudo-likelihood = -423128.29, time = 0.00s\n",
            "[BernoulliRBM] Iteration 93, pseudo-likelihood = -138035.99, time = 0.00s\n",
            "[BernoulliRBM] Iteration 94, pseudo-likelihood = -362803.30, time = 0.00s\n",
            "[BernoulliRBM] Iteration 95, pseudo-likelihood = -187663.63, time = 0.00s\n",
            "[BernoulliRBM] Iteration 96, pseudo-likelihood = -661141.15, time = 0.00s\n",
            "[BernoulliRBM] Iteration 97, pseudo-likelihood = -251795.19, time = 0.00s\n",
            "[BernoulliRBM] Iteration 98, pseudo-likelihood = -1073223.21, time = 0.00s\n",
            "[BernoulliRBM] Iteration 99, pseudo-likelihood = -249501.84, time = 0.00s\n",
            "[BernoulliRBM] Iteration 100, pseudo-likelihood = -653966.79, time = 0.01s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCN15xmcAcoK",
        "colab_type": "code",
        "outputId": "fd3588b7-e02c-443a-f729-947df5ba44ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "W = np.load(\"rbm_weights.npy\")\n",
        "\n",
        "W.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WpR9HTtcAcoN",
        "colab_type": "code",
        "outputId": "8c43de87-e7b3-452e-daa0-b0f21dbdfcd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "%matplotlib inline\n",
        "weightmap_shape = (6,1)\n",
        "fig,axes = plt.subplots(10,20, figsize=(16,8))\n",
        "fig.suptitle('200 components extracted by RBM', fontsize=16)\n",
        "fig.subplots_adjust(hspace = 0.1)\n",
        "for i,ax in enumerate(axes.flatten()):\n",
        "    \n",
        "    ax.imshow(W[i].reshape(weightmap_shape),cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA28AAAH+CAYAAAALajYzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKAByhsAAEADlDcAAIAGKG8AAAAN+P8BsPTcx8IVcQ4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1152x576 with 200 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-BWGnKAcoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save hidden state biases and activations\n",
        "# The activations are the latent features we need\n",
        "np.save(\"/rbm_biases.npy\",rbm.intercept_hidden_)\n",
        "np.save(\"/rbmm_hidden.npy\",rbm.transform(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0wF8BIAcoS",
        "colab_type": "code",
        "outputId": "e45eb346-22ff-4b75-a2c1-d367a3b21ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Run logistic regression on the transformed data, i.e the latent features\n",
        "\n",
        "logrbm = linear_model.LogisticRegression(C=1.0)\n",
        "\n",
        "h = rbm.transform(data)\n",
        "logrbm.fit(h,trainy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hz6cs_cAcoV",
        "colab_type": "code",
        "outputId": "2b61ddea-8205-4479-e0c3-ed366f070fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(classification_report(testy, logrbm.predict(rbm.transform(testx))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      0.41      0.46        41\n",
            "         1.0       0.49      0.59      0.53        39\n",
            "\n",
            "    accuracy                           0.50        80\n",
            "   macro avg       0.50      0.50      0.50        80\n",
            "weighted avg       0.50      0.50      0.50        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvJUXuxAcoY",
        "colab_type": "code",
        "outputId": "b2eb72bf-8597-4dad-d6bb-f6a12a22a9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "rbm_biases = np.load(\"/rbm_biases.npy\")\n",
        "rbm_weights = np.load(\"/rbm_weights.npy\")\n",
        "\n",
        "print(rbm_biases.shape, rbm_weights.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200,) (200, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4NaV1mSAcoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rbm_weight = rbm_weights.T\n",
        "rbm_bias = rbm_biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-nkZg-YAcof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# variables for dimensions \n",
        "feature_size = 200\n",
        "img_size = 6*1\n",
        "num_classes = 2\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None, img_size])\n",
        "\n",
        "# Placeholders for labels\n",
        "y_true = tf.placeholder(tf.float32, [None,num_classes])\n",
        "y_true_cls = tf.placeholder(tf.int64,[None])\n",
        "\n",
        "# Connections between input layer and first hidden layer\n",
        "# These will be initialised with RBM weights and biases\n",
        "\n",
        "rbm_w = tf.Variable(rbm_weight.astype(np.float32))\n",
        "rbm_b = tf.Variable(rbm_bias.astype(np.float32))\n",
        "_h = tf.nn.relu(tf.matmul(x, rbm_w) + rbm_b)\n",
        "\n",
        "\n",
        "# Connections from hidden layer to final output layer\n",
        "weights = tf.Variable(tf.random_normal([feature_size, num_classes],stddev=0.35))\n",
        "biases = tf.Variable(tf.zeros([num_classes]))\n",
        "\n",
        "logits = tf.matmul(_h,weights) + biases\n",
        "print(logits.shape)\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
        "\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                        labels=y_true)\n",
        "\n",
        "cost = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akEu4dW5Acoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "mytrainy =  np.genfromtxt(\"/trainlabel.csv\", delimiter=\",\")\n",
        "\n",
        "def optimize(num_iterations):\n",
        "    for i in range(num_iterations):\n",
        "        train_x, train_y = trainx, mytrainy\n",
        "        train_y=np.concatenate((train_y,mytrainy))\n",
        "        train_y=np.reshape(train_y, (120, 2))\n",
        "        print(train_y.shape)\n",
        "        \n",
        "        feed_dict_for_training = {x:train_x,\n",
        "                                  y_true: train_y}\n",
        "        \n",
        "        \n",
        "        sess.run(optimizer,feed_dict=feed_dict_for_training)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvi5WmqbAcok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF3Um3mjAcon",
        "colab_type": "code",
        "outputId": "bed87a4f-aeed-4c34-a212-447245d03931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "feed_dict_for_testing = { x: testx,\n",
        "                          y_true_cls : testy}\n",
        "print(testx.shape,testy.shape)\n",
        "def get_accuracy():\n",
        "    acc = sess.run(accuracy, feed_dict=feed_dict_for_testing)\n",
        "    \n",
        "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 6) (80,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "92f5da87-5b93-49e3-925a-17e723d3f269",
        "id": "Wx4X5dLSyzv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "get_accuracy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test-set: 50.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y1tHBXrAcou",
        "colab_type": "code",
        "outputId": "dd27963b-286a-4e00-9276-8dd66080e35f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "optimize(num_iterations = 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n",
            "(120, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-R1zIrwAcow",
        "colab_type": "code",
        "outputId": "74fa62f0-94af-46fb-d9ad-4adda9fad5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "get_accuracy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test-set: 52.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsYhL9k5Aco0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize(num_iterations = 90)\n",
        "get_accuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewMRbRqSAco4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize(num_iterations = 100)\n",
        "get_accuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxfKLJKjAco8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize(num_iterations = 100)\n",
        "get_accuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0aQLY3SAco_",
        "colab_type": "code",
        "outputId": "03213f70-1657-4e74-ceb4-0af1d1f1acdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=6, name='rbm'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# set weights between input and hidden layer and hidden biases\n",
        "layer = model.get_layer('rbm')\n",
        "layer.set_weights([rbm_weight,rbm_bias])\n",
        "\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(trainx, trainy, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "120/120 [==============================] - 4s 35ms/step - loss: 8.4413 - acc: 0.4750\n",
            "Epoch 2/10\n",
            "120/120 [==============================] - 0s 134us/step - loss: 8.0648 - acc: 0.5000\n",
            "Epoch 3/10\n",
            "120/120 [==============================] - 0s 131us/step - loss: 8.0648 - acc: 0.5000\n",
            "Epoch 4/10\n",
            "120/120 [==============================] - 0s 111us/step - loss: 8.0648 - acc: 0.5000\n",
            "Epoch 5/10\n",
            "120/120 [==============================] - 0s 120us/step - loss: 8.0648 - acc: 0.5000\n",
            "Epoch 6/10\n",
            "120/120 [==============================] - 0s 102us/step - loss: 8.0647 - acc: 0.5000\n",
            "Epoch 7/10\n",
            "120/120 [==============================] - 0s 126us/step - loss: 8.0647 - acc: 0.5000\n",
            "Epoch 8/10\n",
            "120/120 [==============================] - 0s 111us/step - loss: 8.0647 - acc: 0.5000\n",
            "Epoch 9/10\n",
            "120/120 [==============================] - 0s 118us/step - loss: 8.0647 - acc: 0.5000\n",
            "Epoch 10/10\n",
            "120/120 [==============================] - 0s 103us/step - loss: 8.0647 - acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde10143e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 379
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CIXsUCNAcpB",
        "colab_type": "code",
        "outputId": "a3f760a5-343c-4cd8-bdd8-ae57ae79f2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#print(model.predict(testx))\n",
        "print(classification_report(testy, np.argmax(model.predict(testx),axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.02      0.05        41\n",
            "         1.0       0.49      1.00      0.66        39\n",
            "\n",
            "    accuracy                           0.50        80\n",
            "   macro avg       0.75      0.51      0.35        80\n",
            "weighted avg       0.75      0.50      0.35        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O33m8ShgAcpE",
        "colab_type": "code",
        "outputId": "8173ab18-ff8e-4d94-ed28-132e77f22615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=6))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(trainx, mytrainy, epochs=100, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 2s 20ms/step - loss: 0.9602 - acc: 0.5000\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 1.0052 - acc: 0.5750\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 1.0510 - acc: 0.4917\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 1.2048 - acc: 0.5750\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 95us/step - loss: 1.1263 - acc: 0.5500\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.7996 - acc: 0.6000\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.8302 - acc: 0.5833\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.6899 - acc: 0.7250\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.6909 - acc: 0.6833\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.6272 - acc: 0.6917\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 95us/step - loss: 0.5793 - acc: 0.7417\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.6063 - acc: 0.6917\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.5563 - acc: 0.6417\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.5436 - acc: 0.7083\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.5094 - acc: 0.7167\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.5687 - acc: 0.7167\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.6352 - acc: 0.7167\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.5164 - acc: 0.7500\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.5465 - acc: 0.7250\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.4967 - acc: 0.7083\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.4874 - acc: 0.7667\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 122us/step - loss: 0.4779 - acc: 0.8000\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4843 - acc: 0.7917\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.4664 - acc: 0.8000\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.4799 - acc: 0.7417\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 129us/step - loss: 0.4625 - acc: 0.7667\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4509 - acc: 0.8167\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4717 - acc: 0.8083\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4424 - acc: 0.8167\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.4355 - acc: 0.8000\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4401 - acc: 0.8167\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.4334 - acc: 0.8250\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.4528 - acc: 0.7750\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.4330 - acc: 0.8250\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.4284 - acc: 0.8083\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4062 - acc: 0.8417\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.4397 - acc: 0.8333\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.3957 - acc: 0.8417\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.4202 - acc: 0.8333\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.4159 - acc: 0.8500\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4378 - acc: 0.8083\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 141us/step - loss: 0.4899 - acc: 0.8000\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.4426 - acc: 0.7750\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 86us/step - loss: 0.4370 - acc: 0.8250\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.4575 - acc: 0.8250\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.3881 - acc: 0.8167\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.3964 - acc: 0.8083\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4026 - acc: 0.8250\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.4033 - acc: 0.8750\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.3984 - acc: 0.8083\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.3930 - acc: 0.8500\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.3870 - acc: 0.8667\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.3830 - acc: 0.8500\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.3893 - acc: 0.8333\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.4107 - acc: 0.8417\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.4366 - acc: 0.8250\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.3817 - acc: 0.8500\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 124us/step - loss: 0.3731 - acc: 0.8750\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3594 - acc: 0.8500\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.3477 - acc: 0.8583\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3642 - acc: 0.8750\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3932 - acc: 0.8250\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 95us/step - loss: 0.3819 - acc: 0.8500\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 109us/step - loss: 0.3409 - acc: 0.9000\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 83us/step - loss: 0.3361 - acc: 0.9083\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 87us/step - loss: 0.3438 - acc: 0.8917\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 132us/step - loss: 0.3400 - acc: 0.8583\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.3234 - acc: 0.9333\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.3253 - acc: 0.9000\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 81us/step - loss: 0.3216 - acc: 0.8917\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.3068 - acc: 0.9000\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 115us/step - loss: 0.3208 - acc: 0.8750\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.3228 - acc: 0.9083\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.2997 - acc: 0.9083\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 120us/step - loss: 0.3191 - acc: 0.9083\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3190 - acc: 0.8750\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3348 - acc: 0.9000\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.3061 - acc: 0.9417\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 116us/step - loss: 0.3096 - acc: 0.9250\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.3442 - acc: 0.8667\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.2967 - acc: 0.9000\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.2874 - acc: 0.9333\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.2913 - acc: 0.9000\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.2911 - acc: 0.9250\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.3214 - acc: 0.8917\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.3089 - acc: 0.9083\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 140us/step - loss: 0.3722 - acc: 0.9083\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3028 - acc: 0.8917\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 117us/step - loss: 0.3354 - acc: 0.8667\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 114us/step - loss: 0.2823 - acc: 0.9333\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.2831 - acc: 0.9250\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3142 - acc: 0.8500\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.2879 - acc: 0.9083\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.2792 - acc: 0.9000\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.3164 - acc: 0.8833\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 131us/step - loss: 0.3060 - acc: 0.9083\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.2831 - acc: 0.9000\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.2744 - acc: 0.9167\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.2641 - acc: 0.9250\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2511 - acc: 0.9583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde107bd6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 360
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_4LJDCfAcpH",
        "colab_type": "code",
        "outputId": "5ed36f54-a0c7-41f2-a4e2-f8a5b97f30b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(classification_report(testy, np.argmax(model.predict(testx),axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.90      0.94        41\n",
            "         1.0       0.90      0.97      0.94        39\n",
            "\n",
            "    accuracy                           0.94        80\n",
            "   macro avg       0.94      0.94      0.94        80\n",
            "weighted avg       0.94      0.94      0.94        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZrevhjMeOFM",
        "colab_type": "code",
        "outputId": "9e0a2e49-2cb8-4a70-def8-0c64eac91dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HcZMcyPlAcpR",
        "colab_type": "code",
        "outputId": "5b6b9624-e0f1-45c0-92a9-4d626cb8e5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(200, activation='relu', input_dim=6))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Generate dummy data\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(trainx, mytrainy, epochs=100, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 23ms/step - loss: 1.2477 - acc: 0.4750\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 1.2649 - acc: 0.5417\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 1.0197 - acc: 0.5500\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.8951 - acc: 0.6583\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.7654 - acc: 0.7000\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.9957 - acc: 0.5917\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.5925 - acc: 0.6833\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.9532 - acc: 0.6500\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.7054 - acc: 0.6500\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.7355 - acc: 0.6500\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.6588 - acc: 0.7000\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.6431 - acc: 0.6833\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.6063 - acc: 0.6500\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.5973 - acc: 0.7000\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.5715 - acc: 0.7000\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.5923 - acc: 0.7250\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.5642 - acc: 0.7083\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.5156 - acc: 0.7500\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.4984 - acc: 0.7833\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.4726 - acc: 0.7750\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.4962 - acc: 0.7500\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.5248 - acc: 0.7417\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.5206 - acc: 0.7833\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.4742 - acc: 0.7500\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.4596 - acc: 0.7833\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.4517 - acc: 0.8250\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 82us/step - loss: 0.4635 - acc: 0.7833\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.4680 - acc: 0.7833\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.4421 - acc: 0.8333\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.4660 - acc: 0.7917\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.4664 - acc: 0.8083\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.4456 - acc: 0.8000\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4735 - acc: 0.7833\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 117us/step - loss: 0.4333 - acc: 0.8167\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 114us/step - loss: 0.4251 - acc: 0.8250\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.4213 - acc: 0.8250\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.4425 - acc: 0.8167\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 116us/step - loss: 0.4582 - acc: 0.8000\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 122us/step - loss: 0.4575 - acc: 0.8000\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.4134 - acc: 0.8667\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 141us/step - loss: 0.4152 - acc: 0.8167\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 120us/step - loss: 0.4185 - acc: 0.8167\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.3960 - acc: 0.8500\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.4149 - acc: 0.8250\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.4358 - acc: 0.8167\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.4123 - acc: 0.8167\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3871 - acc: 0.8417\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.3823 - acc: 0.8333\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.3980 - acc: 0.8667\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.3809 - acc: 0.8333\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 137us/step - loss: 0.3746 - acc: 0.8667\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 115us/step - loss: 0.3627 - acc: 0.8833\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3741 - acc: 0.8833\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.3747 - acc: 0.8750\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.3872 - acc: 0.8167\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.3766 - acc: 0.8583\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.3763 - acc: 0.8333\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.3995 - acc: 0.8250\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.4243 - acc: 0.8417\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4089 - acc: 0.8417\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 149us/step - loss: 0.3996 - acc: 0.8083\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3554 - acc: 0.8417\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.3805 - acc: 0.8083\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 146us/step - loss: 0.4090 - acc: 0.8250\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.3914 - acc: 0.8250\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 114us/step - loss: 0.4285 - acc: 0.8083\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4050 - acc: 0.8167\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.3460 - acc: 0.8500\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4325 - acc: 0.8417\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3582 - acc: 0.8250\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 147us/step - loss: 0.3805 - acc: 0.8333\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3366 - acc: 0.8917\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 120us/step - loss: 0.3343 - acc: 0.8750\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.3183 - acc: 0.8750\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 137us/step - loss: 0.3038 - acc: 0.9250\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.3046 - acc: 0.9083\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.3051 - acc: 0.9250\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 122us/step - loss: 0.3028 - acc: 0.9083\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 123us/step - loss: 0.3039 - acc: 0.9083\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.3441 - acc: 0.8833\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3190 - acc: 0.8750\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 120us/step - loss: 0.3184 - acc: 0.9000\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3280 - acc: 0.8667\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.3307 - acc: 0.8917\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.3058 - acc: 0.8750\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.2871 - acc: 0.9167\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.2967 - acc: 0.8917\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.2883 - acc: 0.9167\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.2758 - acc: 0.9167\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.2640 - acc: 0.9583\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.2726 - acc: 0.9583\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.2601 - acc: 0.9417\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.2728 - acc: 0.9083\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2700 - acc: 0.9333\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.3007 - acc: 0.8917\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.2896 - acc: 0.9167\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.3061 - acc: 0.9167\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.3059 - acc: 0.8750\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.3822 - acc: 0.8583\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.2951 - acc: 0.9083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde10744710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZo-X2g8IGIc",
        "colab_type": "code",
        "outputId": "4e2ee4e1-d9d7-4a82-f1c4-b4f99ece81ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(classification_report(testy, np.argmax(model.predict(testx),axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      1.00      0.90        41\n",
            "         1.0       1.00      0.77      0.87        39\n",
            "\n",
            "    accuracy                           0.89        80\n",
            "   macro avg       0.91      0.88      0.89        80\n",
            "weighted avg       0.91      0.89      0.89        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRekDMvfnSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####DBN----\n",
        "import numpy as np\n",
        "from sklearn import linear_model, datasets\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "import os\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14_oppAEIM8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0FpkcWs5hnAN",
        "colab": {}
      },
      "source": [
        "class DBN():\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    train_data,\n",
        "    targets, \n",
        "    layers,\n",
        "    outputs,\n",
        "    rbm_lr,\n",
        "    rbm_iters,\n",
        "    rbm_dir=None,\n",
        "    test_data = None,\n",
        "    test_targets = None,    \n",
        "    epochs = 100,\n",
        "    fine_tune_batch_size = 32\n",
        "\n",
        "     ):\n",
        "\n",
        "    self.hidden_sizes = layers\n",
        "    self.outputs = outputs\n",
        "    self.targets = targets\n",
        "    self.data = train_data\n",
        "\n",
        "    if test_data is None:\n",
        "      self.validate = False\n",
        "    else:\n",
        "      self.validate = True\n",
        "\n",
        "    self.valid_data = test_data\n",
        "    self.valid_labels = test_targets\n",
        "\n",
        "    self.rbm_learning_rate = rbm_lr\n",
        "    self.rbm_iters = rbm_iters\n",
        "\n",
        "    self.epochs = epochs\n",
        "    self.nn_batch_size = fine_tune_batch_size\n",
        "\n",
        "    self.rbm_weights = []\n",
        "    self.rbm_biases = []\n",
        "    self.rbm_h_act = []\n",
        "\n",
        "    self.model = None\n",
        "    self.history = None\n",
        "\n",
        "    \n",
        "    logdir=\"/\"\n",
        "\n",
        "\n",
        "    self.outdir = \"/\"\n",
        "    self.logdir=logdir\n",
        "\n",
        "  def pretrain(self,save=True):\n",
        "    \n",
        "    visual_layer = self.data\n",
        "\n",
        "    for i in range(len(self.hidden_sizes)):\n",
        "      print(\"[DBN] Layer {} Pre-Training\".format(i+1))\n",
        "\n",
        "      rbm = BernoulliRBM(n_components = self.hidden_sizes[i], n_iter = self.rbm_iters[i], learning_rate = 0.0000000001,  verbose = True, batch_size = 32)\n",
        "      grbm=GBRBM(n_visible=6, n_hidden=200, learning_rate=0.01)\n",
        "      rbm.fit(visual_layer)\n",
        "      self.rbm_weights.append(rbm.components_)\n",
        "      self.rbm_biases.append(rbm.intercept_hidden_)\n",
        "      self.rbm_h_act.append(rbm.transform(visual_layer))\n",
        "\n",
        "      visual_layer = self.rbm_h_act[-1]\n",
        "\n",
        "    if save:\n",
        "      with open(self.outdir + \"rbm_weights.p\", 'wb') as f:\n",
        "        pickle.dump(self.rbm_weights, f)\n",
        "\n",
        "      with open(self.outdir + \"rbm_biases.p\", 'wb') as f:\n",
        "        pickle.dump(self.rbm_biases, f)\n",
        "\n",
        "      with open(self.outdir + \"rbm_hidden.p\", 'wb') as f:\n",
        "        pickle.dump(self.rbm_h_act, f) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def finetune(self):\n",
        "    model = Sequential()\n",
        "    for i in range(len(self.hidden_sizes)):\n",
        "\n",
        "      if i==0:\n",
        "        model.add(Dense(self.hidden_sizes[i], activation='relu', input_dim=self.data.shape[1], name='rbm_{}'.format(i)))\n",
        "      else:\n",
        "        model.add(Dense(self.hidden_sizes[i], activation='relu', name='rbm_{}'.format(i)))\n",
        "\n",
        "\n",
        "    model.add(Dense(self.outputs, activation='softmax'))\n",
        "    model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    for i in range(len(self.hidden_sizes)):\n",
        "      layer = model.get_layer('rbm_{}'.format(i))\n",
        "      layer.set_weights([self.rbm_weights[i].transpose(),self.rbm_biases[i]])\n",
        "\n",
        "\n",
        "    if self.validate:\n",
        "      self.history = model.fit(trainx, trainy, \n",
        "                              epochs = self.epochs, \n",
        "                              batch_size = self.nn_batch_size,\n",
        "                              validation_data=(self.valid_data, self.valid_labels)\n",
        "                              )\n",
        "    else:\n",
        "       self.history = model.fit(trainx, trainy, \n",
        "                              epochs = self.epochs, \n",
        "                              batch_size = self.nn_batch_size\n",
        "                            )     \n",
        "    self.model = model\n",
        "\n",
        "  def report(self, data, labels):\n",
        "    print(classification_report((labels), np.argmax(self.model.predict(data),axis=1)))\n",
        "\n",
        "\n",
        "  def save_model(self,filename):\n",
        "\n",
        "    if self.model is None :\n",
        "      raise ValueError(\"Run finetune() first\")\n",
        "\n",
        "    with open(self.outdir + filename, mode='w', encoding='utf-8') as outfile:\n",
        "\n",
        "      data = {\n",
        "              \"model_config\":self.model.get_config(),\n",
        "              \"loss_acc\": self.history.history\n",
        "          }\n",
        "      json.dump(data, outfile, indent=2)\n",
        "\n",
        "  def load_rbm(self):\n",
        "    try:\n",
        "      self.rbm_weights = pickle.load(\"/rbm_weights.p\")\n",
        "      self.rbm_biases = pickle.load(\"/rbm_biases.p\")\n",
        "      self.rbm_h_act = pickle.load(\"/rbm_hidden.p\")\n",
        "    except:\n",
        "      print(\"No such file or directory.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kijz59xZnoHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T0sPgdbYnod2",
        "outputId": "1ece8b82-cf8c-47ee-f406-d40d4e92ab69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  trainx = np.genfromtxt(\"/trainafter.csv\", delimiter=\",\")\n",
        "  trainy=np.genfromtxt(\"/trainlabel.csv\", delimiter=\",\")\n",
        "  testx =np.genfromtxt(\"/test.csv\", delimiter=\",\")\n",
        "  testy = np.genfromtxt(\"/testlabel.csv\", delimiter=\",\")\n",
        "\n",
        "  dbn = DBN(train_data = trainx, targets = trainy,\n",
        "            layers = [200],\n",
        "            outputs = 2,\n",
        "            rbm_iters = [100],\n",
        "            rbm_lr = [0.01]\n",
        "            )\n",
        "  dbn.pretrain(save=True)\n",
        "  dbn.finetune()\n",
        "  #dbn.save_model(\"/bci_dbn_model.json\")\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DBN] Layer 1 Pre-Training\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -14.62, time = 0.03s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -11.97, time = 0.07s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -14.31, time = 0.07s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -20.23, time = 0.07s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -12.59, time = 0.08s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -16.58, time = 0.07s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -16.82, time = 0.07s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -14.60, time = 0.07s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -18.62, time = 0.07s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -11.82, time = 0.07s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -15.04, time = 0.07s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -10.65, time = 0.07s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -14.37, time = 0.07s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -15.82, time = 0.07s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -20.21, time = 0.07s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -14.63, time = 0.07s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -17.03, time = 0.07s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -13.32, time = 0.07s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -15.00, time = 0.07s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -12.71, time = 0.07s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -16.51, time = 0.07s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -16.03, time = 0.07s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -14.97, time = 0.07s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -15.98, time = 0.07s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -14.69, time = 0.09s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -9.41, time = 0.07s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -14.62, time = 0.07s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -16.90, time = 0.08s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -11.69, time = 0.07s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -12.75, time = 0.07s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -17.99, time = 0.07s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -14.16, time = 0.07s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -11.93, time = 0.07s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -16.19, time = 0.07s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -17.51, time = 0.07s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -15.51, time = 0.07s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -13.57, time = 0.07s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -13.09, time = 0.07s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -12.57, time = 0.07s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -16.04, time = 0.08s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -10.70, time = 0.07s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -13.03, time = 0.07s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -15.70, time = 0.07s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -11.74, time = 0.07s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -12.38, time = 0.07s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -11.85, time = 0.07s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -16.20, time = 0.07s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -10.51, time = 0.07s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -14.46, time = 0.07s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -15.52, time = 0.08s\n",
            "[BernoulliRBM] Iteration 51, pseudo-likelihood = -12.09, time = 0.07s\n",
            "[BernoulliRBM] Iteration 52, pseudo-likelihood = -16.26, time = 0.07s\n",
            "[BernoulliRBM] Iteration 53, pseudo-likelihood = -15.34, time = 0.07s\n",
            "[BernoulliRBM] Iteration 54, pseudo-likelihood = -10.86, time = 0.07s\n",
            "[BernoulliRBM] Iteration 55, pseudo-likelihood = -15.71, time = 0.07s\n",
            "[BernoulliRBM] Iteration 56, pseudo-likelihood = -15.04, time = 0.07s\n",
            "[BernoulliRBM] Iteration 57, pseudo-likelihood = -17.95, time = 0.07s\n",
            "[BernoulliRBM] Iteration 58, pseudo-likelihood = -12.52, time = 0.07s\n",
            "[BernoulliRBM] Iteration 59, pseudo-likelihood = -13.93, time = 0.07s\n",
            "[BernoulliRBM] Iteration 60, pseudo-likelihood = -10.93, time = 0.07s\n",
            "[BernoulliRBM] Iteration 61, pseudo-likelihood = -12.04, time = 0.07s\n",
            "[BernoulliRBM] Iteration 62, pseudo-likelihood = -14.76, time = 0.07s\n",
            "[BernoulliRBM] Iteration 63, pseudo-likelihood = -14.00, time = 0.07s\n",
            "[BernoulliRBM] Iteration 64, pseudo-likelihood = -15.44, time = 0.07s\n",
            "[BernoulliRBM] Iteration 65, pseudo-likelihood = -16.09, time = 0.07s\n",
            "[BernoulliRBM] Iteration 66, pseudo-likelihood = -13.45, time = 0.07s\n",
            "[BernoulliRBM] Iteration 67, pseudo-likelihood = -11.89, time = 0.07s\n",
            "[BernoulliRBM] Iteration 68, pseudo-likelihood = -13.01, time = 0.07s\n",
            "[BernoulliRBM] Iteration 69, pseudo-likelihood = -16.57, time = 0.07s\n",
            "[BernoulliRBM] Iteration 70, pseudo-likelihood = -19.15, time = 0.07s\n",
            "[BernoulliRBM] Iteration 71, pseudo-likelihood = -13.84, time = 0.07s\n",
            "[BernoulliRBM] Iteration 72, pseudo-likelihood = -16.64, time = 0.07s\n",
            "[BernoulliRBM] Iteration 73, pseudo-likelihood = -19.33, time = 0.07s\n",
            "[BernoulliRBM] Iteration 74, pseudo-likelihood = -11.75, time = 0.07s\n",
            "[BernoulliRBM] Iteration 75, pseudo-likelihood = -16.69, time = 0.07s\n",
            "[BernoulliRBM] Iteration 76, pseudo-likelihood = -13.01, time = 0.07s\n",
            "[BernoulliRBM] Iteration 77, pseudo-likelihood = -11.39, time = 0.07s\n",
            "[BernoulliRBM] Iteration 78, pseudo-likelihood = -17.38, time = 0.08s\n",
            "[BernoulliRBM] Iteration 79, pseudo-likelihood = -12.69, time = 0.07s\n",
            "[BernoulliRBM] Iteration 80, pseudo-likelihood = -15.11, time = 0.07s\n",
            "[BernoulliRBM] Iteration 81, pseudo-likelihood = -11.63, time = 0.07s\n",
            "[BernoulliRBM] Iteration 82, pseudo-likelihood = -17.55, time = 0.07s\n",
            "[BernoulliRBM] Iteration 83, pseudo-likelihood = -15.96, time = 0.08s\n",
            "[BernoulliRBM] Iteration 84, pseudo-likelihood = -14.93, time = 0.07s\n",
            "[BernoulliRBM] Iteration 85, pseudo-likelihood = -17.82, time = 0.07s\n",
            "[BernoulliRBM] Iteration 86, pseudo-likelihood = -14.98, time = 0.07s\n",
            "[BernoulliRBM] Iteration 87, pseudo-likelihood = -12.04, time = 0.07s\n",
            "[BernoulliRBM] Iteration 88, pseudo-likelihood = -16.03, time = 0.07s\n",
            "[BernoulliRBM] Iteration 89, pseudo-likelihood = -12.39, time = 0.08s\n",
            "[BernoulliRBM] Iteration 90, pseudo-likelihood = -10.98, time = 0.07s\n",
            "[BernoulliRBM] Iteration 91, pseudo-likelihood = -15.46, time = 0.07s\n",
            "[BernoulliRBM] Iteration 92, pseudo-likelihood = -16.46, time = 0.07s\n",
            "[BernoulliRBM] Iteration 93, pseudo-likelihood = -11.03, time = 0.07s\n",
            "[BernoulliRBM] Iteration 94, pseudo-likelihood = -12.72, time = 0.07s\n",
            "[BernoulliRBM] Iteration 95, pseudo-likelihood = -20.88, time = 0.07s\n",
            "[BernoulliRBM] Iteration 96, pseudo-likelihood = -19.47, time = 0.07s\n",
            "[BernoulliRBM] Iteration 97, pseudo-likelihood = -7.28, time = 0.07s\n",
            "[BernoulliRBM] Iteration 98, pseudo-likelihood = -10.88, time = 0.07s\n",
            "[BernoulliRBM] Iteration 99, pseudo-likelihood = -14.40, time = 0.07s\n",
            "[BernoulliRBM] Iteration 100, pseudo-likelihood = -16.82, time = 0.07s\n",
            "Epoch 1/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0957 - acc: 0.4500\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 115us/step - loss: 0.8695 - acc: 0.5417\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.7635 - acc: 0.6250\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.7344 - acc: 0.6583\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.6703 - acc: 0.6917\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.6179 - acc: 0.6583\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.6086 - acc: 0.7083\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.5825 - acc: 0.7167\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.6027 - acc: 0.6833\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.5722 - acc: 0.7167\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.5638 - acc: 0.7167\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.5996 - acc: 0.6750\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.5579 - acc: 0.7000\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.5535 - acc: 0.7250\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 116us/step - loss: 0.5486 - acc: 0.7250\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.5302 - acc: 0.7500\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 114us/step - loss: 0.5431 - acc: 0.7500\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.5298 - acc: 0.7500\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.5345 - acc: 0.7083\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.5091 - acc: 0.7583\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.5264 - acc: 0.7417\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.5235 - acc: 0.7667\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 115us/step - loss: 0.5071 - acc: 0.7667\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.4936 - acc: 0.7500\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 120us/step - loss: 0.4975 - acc: 0.7583\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.4761 - acc: 0.7833\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.5330 - acc: 0.7250\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.5460 - acc: 0.7167\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.6049 - acc: 0.7333\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.5507 - acc: 0.7000\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.4994 - acc: 0.7333\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.5439 - acc: 0.6833\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.5526 - acc: 0.7583\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.4848 - acc: 0.7917\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.4965 - acc: 0.7500\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.5013 - acc: 0.7583\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.4747 - acc: 0.7583\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 114us/step - loss: 0.4787 - acc: 0.7833\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 122us/step - loss: 0.4800 - acc: 0.7750\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 148us/step - loss: 0.4427 - acc: 0.8167\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.4539 - acc: 0.7583\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.4558 - acc: 0.7750\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.4656 - acc: 0.7667\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4597 - acc: 0.7833\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.5278 - acc: 0.7000\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.5064 - acc: 0.7417\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.5116 - acc: 0.7583\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 109us/step - loss: 0.4583 - acc: 0.7250\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.4263 - acc: 0.8083\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 115us/step - loss: 0.4174 - acc: 0.8250\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.4234 - acc: 0.8250\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.4310 - acc: 0.8083\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 103us/step - loss: 0.3991 - acc: 0.8250\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.4187 - acc: 0.8167\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.4049 - acc: 0.8083\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 82us/step - loss: 0.4027 - acc: 0.8250\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 86us/step - loss: 0.4016 - acc: 0.8333\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.4026 - acc: 0.8167\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4037 - acc: 0.8083\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4772 - acc: 0.7917\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4453 - acc: 0.8250\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.3885 - acc: 0.8417\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4375 - acc: 0.8417\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4592 - acc: 0.7833\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 109us/step - loss: 0.4137 - acc: 0.8333\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4315 - acc: 0.8167\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.3894 - acc: 0.8250\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 116us/step - loss: 0.4058 - acc: 0.8167\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 117us/step - loss: 0.4773 - acc: 0.8000\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.5054 - acc: 0.7500\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.4479 - acc: 0.7833\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4407 - acc: 0.7833\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.4118 - acc: 0.8083\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.4066 - acc: 0.8250\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.4225 - acc: 0.7750\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3976 - acc: 0.8250\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.3985 - acc: 0.8500\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 101us/step - loss: 0.4047 - acc: 0.8583\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.5281 - acc: 0.8083\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.4028 - acc: 0.8000\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.4534 - acc: 0.7250\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.5176 - acc: 0.7917\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.5193 - acc: 0.7333\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.4068 - acc: 0.8167\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3952 - acc: 0.8167\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.4243 - acc: 0.7833\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4004 - acc: 0.8417\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3665 - acc: 0.8583\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 111us/step - loss: 0.3347 - acc: 0.8667\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3533 - acc: 0.8417\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.3581 - acc: 0.8250\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 106us/step - loss: 0.3310 - acc: 0.8833\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.3084 - acc: 0.8917\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 109us/step - loss: 0.3232 - acc: 0.8750\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 107us/step - loss: 0.3174 - acc: 0.8583\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.2988 - acc: 0.8917\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 105us/step - loss: 0.2998 - acc: 0.9167\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 119us/step - loss: 0.2993 - acc: 0.9083\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 121us/step - loss: 0.2991 - acc: 0.8750\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 184us/step - loss: 0.2936 - acc: 0.9083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhTdRw6Jnsvs",
        "colab_type": "code",
        "outputId": "020b2ba1-3998-477f-d37d-96f1bc837953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "  print(\"Training Report\")\n",
        "  dbn.report(trainx,trainy)\n",
        "  \n",
        "\n",
        "  #print((trainx),(trainy),(testx),(testy))\n",
        "  print(\"Testing Report\")\n",
        "  dbn.report(testx,testy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.90      0.91        60\n",
            "         1.0       0.90      0.92      0.91        60\n",
            "\n",
            "    accuracy                           0.91       120\n",
            "   macro avg       0.91      0.91      0.91       120\n",
            "weighted avg       0.91      0.91      0.91       120\n",
            "\n",
            "Testing Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.93      0.92        41\n",
            "         1.0       0.92      0.90      0.91        39\n",
            "\n",
            "    accuracy                           0.91        80\n",
            "   macro avg       0.91      0.91      0.91        80\n",
            "weighted avg       0.91      0.91      0.91        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}